# Heart Disease Risk Predictor
## Data Management for Data Science - Complete Implementation

A complete data science project that demonstrates advanced data management techniques, SQL-based analysis, ETL pipelines, and machine learning for heart disease prediction.

##  Project Overview

This project implements a complete data management and analysis pipeline for heart disease prediction, specifically designed for **DATA MGMT FOR DATASC 01:198:210:G1** course requirements.

### Key Features
- **Database Design**: SQLite schema with proper constraints and indexing
- **SQL Analysis**: 10+ complex queries for exploratory data analysis
- **ETL Pipeline**: Complete extract-transform-load implementation
- **Machine Learning**: Multiple algorithms with hyperparameter tuning
- **Data Visualization**: Complete statistical charts and EDA
- **Academic Documentation**: Complete project structure and analysis

##  Dataset

**Source**: Heart Disease UCI Dataset
**Records**: 303 patients
**Features**: 14 clinical attributes
**Target**: Heart disease presence (binary classification)

### Features Description
- `age`: Age in years
- `sex`: Gender (0 = female, 1 = male)
- `cp`: Chest pain type (0-3)
- `trestbps`: Resting blood pressure
- `chol`: Serum cholesterol
- `fbs`: Fasting blood sugar > 120 mg/dl
- `restecg`: Resting ECG results
- `thalch`: Maximum heart rate achieved
- `exang`: Exercise induced angina
- `oldpeak`: ST depression induced by exercise
- `slope`: Slope of peak exercise ST segment
- `ca`: Number of major vessels colored by fluoroscopy
- `thal`: Thalassemia type
- `num`: Heart disease diagnosis (0-4, >0 indicates disease)

## üèó System Architecture

### 1. Database Layer (`database_setup.py`)
```
‚îú‚îÄ‚îÄ SQLite Database with proper schema
‚îú‚îÄ‚îÄ Data integrity constraints
‚îú‚îÄ‚îÄ Performance indexes
‚îú‚îÄ‚îÄ Automated data validation
‚îî‚îÄ‚îÄ Complete table management
```

**Key Components:**
- **DatabaseManager Class**: Handles all database operations
- **Schema Design**: Proper data types, constraints, and relationships
- **Indexing Strategy**: Optimized for analytical queries
- **Data Integrity**: Automated validation and constraint checking

### 2. SQL Analysis Layer (`sql_analysis.py`)
```
‚îú‚îÄ‚îÄ 10+ Complex SQL Queries
‚îú‚îÄ‚îÄ Exploratory Data Analysis
‚îú‚îÄ‚îÄ Statistical Aggregations
‚îú‚îÄ‚îÄ Multi-factor Risk Analysis
‚îî‚îÄ‚îÄ Data Quality Assessment
```

**Analysis Categories:**
- Basic dataset statistics
- Target variable distribution
- Age group analysis
- Gender-based analysis
- Chest pain type correlation
- Cholesterol level impact
- Blood pressure analysis
- Exercise angina patterns
- High-risk patient identification
- Data quality verification

### 3. ETL Pipeline (`etl_pipeline.py`)
```
‚îú‚îÄ‚îÄ Extract: CSV data ingestion
‚îú‚îÄ‚îÄ Transform: Data cleaning & validation
‚îú‚îÄ‚îÄ Load: Database population
‚îú‚îÄ‚îÄ Analysis: SQL-based exploration
‚îî‚îÄ‚îÄ Reporting: Complete logs
```

**Pipeline Features:**
- **Automated Processing**: Complete data flow automation
- **Error Handling**: Complete exception management
- **Logging System**: Detailed operation tracking
- **Data Validation**: Multi-level quality checks
- **Performance Monitoring**: Processing metrics and statistics

### 4. Machine Learning Layer (`model_pipeline.py`)
```
‚îú‚îÄ‚îÄ Data Preprocessing
‚îú‚îÄ‚îÄ Feature Engineering
‚îú‚îÄ‚îÄ Model Training (Multiple Algorithms)
‚îú‚îÄ‚îÄ Hyperparameter Tuning
‚îî‚îÄ‚îÄ Performance Evaluation
```

**ML Components:**
- Random Forest Classifier
- Logistic Regression
- Gradient Boosting
- Cross-validation
- Feature importance analysis
- ROC curve analysis
- Confusion matrix evaluation

##  Getting Started

### Prerequisites
```bash
Python 3.8+
SQLite3
Git
```

### Installation
```bash
# Clone the repository
git clone https://github.com/AdityaBaranwal1/heart-disease-risk-predictor.git
cd heart-disease-risk-predictor

# Install dependencies
pip install -r requirements.txt
```

### Usage

#### 1. Run Complete ETL Pipeline
```bash
python etl_pipeline.py
```
This will:
- Extract data from CSV
- Transform and clean the data
- Load into SQLite database
- Execute complete SQL analysis
- Generate detailed logs

#### 2. Database Setup (Standalone)
```bash
python database_setup.py
```

#### 3. SQL Analysis (Standalone)
```bash
python sql_analysis.py
```

#### 4. Machine Learning Pipeline
```bash
python model_pipeline.py
```

#### 5. Data Cleaning (Standalone)
```bash
python data_cleaning.py
```

##  Project Structure

```
heart-disease-risk-predictor/
‚îú‚îÄ‚îÄ  Data Management Components
‚îÇ   ‚îú‚îÄ‚îÄ database_setup.py      # SQLite database design & management
‚îÇ   ‚îú‚îÄ‚îÄ sql_analysis.py        # Complete SQL analysis
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.py        # Complete ETL implementation
‚îÇ   ‚îî‚îÄ‚îÄ data_cleaning.py       # Data preprocessing utilities
‚îú‚îÄ‚îÄ ü§ñ Machine Learning Components
‚îÇ   ‚îú‚îÄ‚îÄ model_pipeline.py      # ML model training & evaluation
‚îÇ   ‚îî‚îÄ‚îÄ model_training.ipynb   # Jupyter notebook for analysis
‚îú‚îÄ‚îÄ  Outputs & Results
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png   # Model evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ feature_importance.png # Feature analysis
‚îÇ   ‚îú‚îÄ‚îÄ roc_curve.png          # ROC curve analysis
‚îÇ   ‚îî‚îÄ‚îÄ model.pkl              # Trained model
‚îú‚îÄ‚îÄ  Data & Documentation
‚îÇ   ‚îú‚îÄ‚îÄ heart_disease_data.csv # Source dataset
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_report.md   # Model evaluation report
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.sql       # SQL scripts
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt       # Dependencies
‚îî‚îÄ‚îÄ üìù Configuration
    ‚îú‚îÄ‚îÄ README.md              # This file
    ‚îî‚îÄ‚îÄ .gitignore             # Git ignore rules
```

##  Database Design

### Heart Disease Table Schema
```sql
CREATE TABLE heart_disease (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    age INTEGER NOT NULL CHECK (age > 0 AND age < 120),
    sex INTEGER NOT NULL CHECK (sex IN (0, 1)),
    cp INTEGER NOT NULL CHECK (cp >= 0 AND cp <= 3),
    trestbps INTEGER CHECK (trestbps > 0 AND trestbps < 300),
    chol INTEGER CHECK (chol >= 0),
    fbs INTEGER CHECK (fbs IN (0, 1)),
    restecg INTEGER CHECK (restecg >= 0 AND restecg <= 2),
    thalch INTEGER CHECK (thalch > 0 AND thalch < 250),
    exang TEXT CHECK (exang IN ('TRUE', 'FALSE')),
    oldpeak REAL CHECK (oldpeak >= 0),
    slope INTEGER CHECK (slope >= 0 AND slope <= 2),
    ca INTEGER CHECK (ca >= 0 AND ca <= 4),
    thal INTEGER CHECK (thal >= 0 AND thal <= 3),
    num INTEGER NOT NULL CHECK (num >= 0 AND num <= 4),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Performance Indexes
- `idx_age`: Age-based queries
- `idx_sex`: Gender analysis
- `idx_num`: Target variable queries
- `idx_age_sex`: Combined demographic analysis
- `idx_chol`: Cholesterol-based analysis
- `idx_trestbps`: Blood pressure analysis

##  SQL Analysis Examples

### Age Group Risk Analysis
```sql
SELECT
    CASE
        WHEN age < 40 THEN 'Under 40'
        WHEN age BETWEEN 40 AND 50 THEN '40-50'
        WHEN age BETWEEN 51 AND 60 THEN '51-60'
        ELSE 'Over 60'
    END as age_group,
    COUNT(*) as total_patients,
    SUM(CASE WHEN num > 0 THEN 1 ELSE 0 END) as with_disease,
    ROUND(AVG(CASE WHEN num > 0 THEN 1.0 ELSE 0.0 END) * 100, 2) as disease_rate_percent
FROM heart_disease
GROUP BY age_group
ORDER BY MIN(age);
```

### Multi-Factor Risk Assessment
```sql
SELECT
    CASE
        WHEN age > 55 AND chol > 240 AND trestbps > 140 THEN '3 Risk Factors'
        WHEN (age > 55 AND chol > 240) OR (age > 55 AND trestbps > 140) OR (chol > 240 AND trestbps > 140) THEN '2 Risk Factors'
        WHEN age > 55 OR chol > 240 OR trestbps > 140 THEN '1 Risk Factor'
        ELSE 'No Major Risk Factors'
    END as risk_category,
    COUNT(*) as patient_count,
    SUM(CASE WHEN num > 0 THEN 1 ELSE 0 END) as with_disease,
    ROUND(AVG(CASE WHEN num > 0 THEN 1.0 ELSE 0.0 END) * 100, 2) as disease_rate_percent
FROM heart_disease
GROUP BY risk_category
ORDER BY disease_rate_percent DESC;
```

## ü§ñ Machine Learning Results

### Model Performance
- **Random Forest**: 84% accuracy
- **Logistic Regression**: 86% accuracy
- **Gradient Boosting**: 88% accuracy

### Key Features (by importance)
1. `thalch` (Maximum heart rate)
2. `oldpeak` (ST depression)
3. `cp` (Chest pain type)
4. `age` (Patient age)
5. `ca` (Major vessels)

##  Data Management Achievements

### Academic Requirements Fulfilled
- [x] **Database Design**: Proper SQLite schema with constraints
- [x] **SQL Analysis**: 10+ complex analytical queries
- [x] **ETL Pipeline**: Complete automated data processing
- [x] **Data Quality**: Complete validation and integrity checks
- [x] **Performance Optimization**: Strategic indexing for analytical workloads
- [x] **Documentation**: Complete academic-level documentation
- [x] **Logging & Monitoring**: Detailed operation tracking
- [x] **Error Handling**: Reliable exception management
- [x] **Scalability**: Modular design for extensibility

### Data Management Best Practices
- **ACID Compliance**: Proper transaction management
- **Data Integrity**: Constraint-based validation
- **Query Optimization**: Index-based performance tuning
- **Separation of Concerns**: Modular architecture
- **Complete Logging**: Audit trail for all operations
- **Error Recovery**: Graceful handling of edge cases

##  Technical Specifications

### Dependencies
- **pandas**: Data manipulation and analysis
- **numpy**: Numerical computing
- **scikit-learn**: Machine learning algorithms
- **matplotlib/seaborn**: Data visualization
- **sqlite3**: Database management
- **sqlalchemy**: Database ORM
- **jupyter**: Interactive development

### System Requirements
- **Python**: 3.8 or higher
- **Memory**: 4GB RAM minimum
- **Storage**: 100MB for datasets and models
- **OS**: Windows, macOS, or Linux

## üìù Academic Context

### Course: DATA MGMT FOR DATASC 01:198:210:G1
This project demonstrates mastery of:
- **Database Design Principles**
- **SQL Query Optimization**
- **ETL Pipeline Development**
- **Data Quality Management**
- **Performance Monitoring**
- **Academic Documentation Standards**

### Grading Criteria Addressed
-  Database schema design and implementation
-  Complex SQL query development
-  ETL pipeline architecture
-  Data integrity and validation
-  Performance optimization
-  Complete documentation
-  Code quality and organization

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/enhancement`)
3. Commit changes (`git commit -am 'Add enhancement'`)
4. Push to branch (`git push origin feature/enhancement`)
5. Create Pull Request

##  License

This project is developed for academic purposes as part of the Data Management for Data Science course.

## üôè Acknowledgments

- UCI Machine Learning Repository for the heart disease dataset
- Course instructors for Data Management guidance
- Open source community for tools and libraries

---

**Note**: This implementation represents a complete data management system suitable for academic evaluation and real-world application. All components are designed to demonstrate best practices in database design, SQL analysis, ETL development, and data science workflows.
